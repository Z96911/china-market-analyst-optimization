"""
Prompt æ•ˆæœè¯„ä¼°å·¥å…·
==================

ç”¨äºè¯„ä¼°ä¼˜åŒ–åçš„ Prompt ç›¸æ¯”åŸç‰ˆçš„æ”¹è¿›æ•ˆæœã€‚

ä½¿ç”¨æ–¹æ³•ï¼š
1. å°†æ­¤æ–‡ä»¶æ”¾åˆ°é¡¹ç›®æ ¹ç›®å½•
2. è¿è¡Œ: python prompt_evaluator.py

è¯„ä¼°ç»´åº¦ï¼š
1. è¾“å‡ºè´¨é‡ï¼ˆå®Œæ•´æ€§ã€å‡†ç¡®æ€§ï¼‰
2. Token æ•ˆç‡ï¼ˆè¾“å…¥/è¾“å‡º token æ•°ï¼‰
3. å“åº”æ—¶é—´
4. å†³ç­–æ•ˆæœï¼ˆå›æµ‹æ”¶ç›Šï¼‰
"""

import json
import time
import hashlib
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
import pandas as pd
from langchain_core.messages import HumanMessage, ToolMessage


@dataclass
class EvaluationResult:
    """å•æ¬¡è¯„ä¼°ç»“æœ"""
    test_case_id: str
    prompt_version: str
    ticker: str
    date: str
    
    # è´¨é‡æŒ‡æ ‡
    completeness_score: float      # å†…å®¹å®Œæ•´æ€§ (0-1)
    format_compliance: float       # æ ¼å¼ç¬¦åˆåº¦ (0-1)
    data_accuracy: float           # æ•°æ®å‡†ç¡®æ€§ (0-1)
    
    # æ•ˆç‡æŒ‡æ ‡
    input_tokens: int
    output_tokens: int
    response_time_ms: int
    
    # å†³ç­–æŒ‡æ ‡
    recommendation: str            # ä¹°å…¥/æŒæœ‰/å–å‡º
    confidence: float              # ç½®ä¿¡åº¦ (0-1)
    
    # å®é™…ç»“æœï¼ˆå›æµ‹æ—¶å¡«å……ï¼‰
    actual_return_5d: Optional[float] = None
    actual_return_10d: Optional[float] = None
    
    def to_dict(self):
        return asdict(self)


class PromptEvaluator:
    """Prompt æ•ˆæœè¯„ä¼°å™¨"""
    
    # æµ‹è¯•ç”¨ä¾‹ï¼šè¦†ç›–ä¸åŒç±»å‹çš„è‚¡ç¥¨
    DEFAULT_TEST_CASES = [
        # å¤§ç›˜è“ç­¹
        {"ticker": "600519.SH", "name": "è´µå·èŒ…å°", "type": "ç™½é…’é¾™å¤´"},
        {"ticker": "601318.SH", "name": "ä¸­å›½å¹³å®‰", "type": "ä¿é™©é¾™å¤´"},
        {"ticker": "000858.SZ", "name": "äº”ç²®æ¶²", "type": "ç™½é…’"},
        
        # ç§‘æŠ€æˆé•¿
        {"ticker": "300750.SZ", "name": "å®å¾·æ—¶ä»£", "type": "æ–°èƒ½æº"},
        {"ticker": "002415.SZ", "name": "æµ·åº·å¨è§†", "type": "å®‰é˜²"},
        
        # å‘¨æœŸè‚¡
        {"ticker": "601899.SH", "name": "ç´«é‡‘çŸ¿ä¸š", "type": "æœ‰è‰²é‡‘å±"},
        {"ticker": "600028.SH", "name": "ä¸­å›½çŸ³åŒ–", "type": "çŸ³æ²¹"},
        
        # ä¸­å°ç›˜
        {"ticker": "300059.SZ", "name": "ä¸œæ–¹è´¢å¯Œ", "type": "åˆ¸å•†äº’è”ç½‘"},
        {"ticker": "002594.SZ", "name": "æ¯”äºšè¿ª", "type": "æ–°èƒ½æºæ±½è½¦"},
        
        # ç‰¹æ®Šæƒ…å†µ
        {"ticker": "000001.SZ", "name": "å¹³å®‰é“¶è¡Œ", "type": "é“¶è¡Œ"},
    ]
    
    # å¿…é¡»åŒ…å«çš„å…³é”®å†…å®¹ï¼ˆç”¨äºæ£€æŸ¥å®Œæ•´æ€§ï¼‰
    REQUIRED_SECTIONS = {
        "quick": [
            "æŠ•èµ„è¯„çº§",
            "æ ¸å¿ƒé€»è¾‘", 
            "å…³é”®æ•°æ®",
            "é£é™©",
        ],
        "deep": [
            "å¸‚åœºç¯å¢ƒ",
            "åŸºæœ¬é¢",
            "æŠ€æœ¯é¢",
            "èµ„é‡‘é¢",
            "æŠ•èµ„å»ºè®®",
        ]
    }
    
    def __init__(self, config: Dict = None):
        self.config = config or {}
        self.results: List[EvaluationResult] = []
        
    def check_completeness(self, output: str, mode: str = "quick") -> float:
        """
        æ£€æŸ¥è¾“å‡ºçš„å®Œæ•´æ€§
        
        Args:
            output: LLM è¾“å‡ºå†…å®¹
            mode: "quick" æˆ– "deep"
            
        Returns:
            å®Œæ•´æ€§å¾—åˆ† (0-1)
        """
        required = self.REQUIRED_SECTIONS.get(mode, self.REQUIRED_SECTIONS["quick"])
        found = sum(1 for section in required if section in output)
        return found / len(required)
    
    def check_format_compliance(self, output: str, mode: str = "quick") -> float:
        """
        æ£€æŸ¥è¾“å‡ºæ ¼å¼æ˜¯å¦ç¬¦åˆè¦æ±‚
        
        Args:
            output: LLM è¾“å‡ºå†…å®¹
            mode: åˆ†ææ¨¡å¼
            
        Returns:
            æ ¼å¼ç¬¦åˆåº¦å¾—åˆ† (0-1)
        """
        score = 0.0
        checks = []
        
        if mode == "quick":
            # å¿«é€Ÿæ¨¡å¼çš„æ ¼å¼æ£€æŸ¥
            checks = [
                ("##" in output, 0.2),                    # æœ‰æ ‡é¢˜
                ("â­" in output or "æ˜Ÿ" in output, 0.2),   # æœ‰è¯„çº§
                ("|" in output and "---" in output, 0.3), # æœ‰è¡¨æ ¼
                (len(output) < 1500, 0.3),                # ç®€æ´æ€§ï¼ˆ<1500å­—ç¬¦ï¼‰
            ]
        else:
            # æ·±åº¦æ¨¡å¼çš„æ ¼å¼æ£€æŸ¥
            checks = [
                ("##" in output, 0.15),                   # æœ‰ç« èŠ‚æ ‡é¢˜
                ("è¯„åˆ†" in output or "åˆ†" in output, 0.2), # æœ‰è¯„åˆ†
                ("|" in output, 0.15),                    # æœ‰è¡¨æ ¼
                ("æ­¢ç›ˆ" in output or "æ­¢æŸ" in output, 0.2), # æœ‰æ“ä½œå»ºè®®
                (len(output) > 800, 0.3),                 # å……åˆ†æ€§ï¼ˆ>800å­—ç¬¦ï¼‰
            ]
        
        for condition, weight in checks:
            if condition:
                score += weight
                
        return min(score, 1.0)
    
    def check_data_accuracy(self, output: str, ground_truth: Dict) -> float:
        """
        æ£€æŸ¥æ•°æ®å‡†ç¡®æ€§
        
        Args:
            output: LLM è¾“å‡ºå†…å®¹
            ground_truth: çœŸå®æ•°æ®å­—å…¸ {"PE": 30, "ROE": 25, ...}
            
        Returns:
            æ•°æ®å‡†ç¡®æ€§å¾—åˆ† (0-1)
        """
        if not ground_truth:
            return 0.5  # æ²¡æœ‰çœŸå®æ•°æ®æ—¶è¿”å›ä¸­æ€§åˆ†æ•°
        
        correct = 0
        total = len(ground_truth)
        
        for key, true_value in ground_truth.items():
            # åœ¨è¾“å‡ºä¸­æŸ¥æ‰¾è¯¥æŒ‡æ ‡çš„å€¼
            # è¿™é‡Œä½¿ç”¨ç®€åŒ–çš„åŒ¹é…é€»è¾‘ï¼Œå®é™…ä½¿ç”¨æ—¶å¯èƒ½éœ€è¦æ›´å¤æ‚çš„è§£æ
            if str(true_value) in output or key in output:
                correct += 0.5  # éƒ¨åˆ†åˆ†
                # TODO: æ›´ç²¾ç¡®çš„æ•°å€¼æ¯”å¯¹
                
        return correct / total if total > 0 else 0.5
    
    def extract_recommendation(self, output: str) -> tuple:
        """
        ä»è¾“å‡ºä¸­æå–æŠ•èµ„å»ºè®®
        
        Returns:
            (recommendation, confidence)
        """
        recommendation = "æŒæœ‰"  # é»˜è®¤
        confidence = 0.5
        
        # ç®€å•çš„å…³é”®è¯åŒ¹é…
        if "å¼ºçƒˆæ¨è" in output or "ä¹°å…¥" in output or "â­â­â­â­â­" in output:
            recommendation = "ä¹°å…¥"
            confidence = 0.8
        elif "æ¨è" in output or "â­â­â­â­" in output:
            recommendation = "ä¹°å…¥"
            confidence = 0.6
        elif "å›é¿" in output or "å–å‡º" in output or "â­" in output:
            recommendation = "å–å‡º"
            confidence = 0.7
        elif "ä¸­æ€§" in output or "è§‚æœ›" in output:
            recommendation = "æŒæœ‰"
            confidence = 0.5
            
        return recommendation, confidence
    
    def _execute_tool_calls(self, tool_calls, toolkit):
        """æ‰§è¡Œå·¥å…·è°ƒç”¨å¹¶è¿”å›ç»“æœ"""
        tool_results = []

        # å·¥å…·åç§°åˆ°å‡½æ•°çš„æ˜ å°„
        tool_map = {
            "get_china_stock_data": toolkit.get_china_stock_data,
            "get_china_market_overview": toolkit.get_china_market_overview,
            "get_YFin_data": toolkit.get_YFin_data,
        }

        for tc in tool_calls:
            tool_name = tc.get("name", "")
            tool_args = tc.get("args", {})
            tool_id = tc.get("id", "")

            if tool_name in tool_map:
                try:
                    tool_func = tool_map[tool_name]
                    if hasattr(tool_func, 'invoke'):
                        result = tool_func.invoke(tool_args)
                    else:
                        result = tool_func(**tool_args)

                    tool_results.append(ToolMessage(
                        content=str(result)[:5000],
                        tool_call_id=tool_id
                    ))
                except Exception as e:
                    tool_results.append(ToolMessage(
                        content=f"å·¥å…·è°ƒç”¨å¤±è´¥: {e}",
                        tool_call_id=tool_id
                    ))
            else:
                tool_results.append(ToolMessage(
                    content=f"æœªçŸ¥å·¥å…·: {tool_name}",
                    tool_call_id=tool_id
                ))

        return tool_results

    def run_single_evaluation(
        self,
        analyst_func,
        ticker: str,
        date: str,
        prompt_version: str,
        mode: str = "quick",
        ground_truth: Dict = None,
        toolkit = None
    ) -> EvaluationResult:
        """
        è¿è¡Œå•æ¬¡è¯„ä¼°

        Args:
            analyst_func: åˆ†æå¸ˆå‡½æ•°
            ticker: è‚¡ç¥¨ä»£ç 
            date: åˆ†ææ—¥æœŸ
            prompt_version: Prompt ç‰ˆæœ¬æ ‡è¯†
            mode: åˆ†ææ¨¡å¼
            ground_truth: çœŸå®æ•°æ®ï¼ˆç”¨äºå‡†ç¡®æ€§æ£€éªŒï¼‰
            toolkit: å·¥å…·é›†ï¼ˆç”¨äºæ‰§è¡Œå·¥å…·è°ƒç”¨ï¼‰

        Returns:
            EvaluationResult
        """
        # æ„é€ è¾“å…¥çŠ¶æ€
        state = {
            "trade_date": date,
            "company_of_interest": ticker,
            "messages": [HumanMessage(content=f"è¯·åˆ†æè‚¡ç¥¨ {ticker}")],
        }

        # è®¡æ—¶
        start_time = time.time()

        # è°ƒç”¨åˆ†æå¸ˆ
        try:
            result = analyst_func(state)
            output = result.get("china_market_report", "")

            # å¦‚æœæ²¡æœ‰æŠ¥å‘Šï¼Œæ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨éœ€è¦æ‰§è¡Œ
            if not output and toolkit:
                messages = result.get("messages", [])
                if messages and hasattr(messages[0], "tool_calls") and messages[0].tool_calls:
                    # æ‰§è¡Œå·¥å…·è°ƒç”¨
                    tool_results = self._execute_tool_calls(messages[0].tool_calls, toolkit)

                    # æ›´æ–° state å¹¶å†æ¬¡è°ƒç”¨åˆ†æå¸ˆ
                    state["messages"] = state["messages"] + [messages[0]] + tool_results
                    result = analyst_func(state)
                    output = result.get("china_market_report", "")

                    # å¦‚æœè¿˜æ˜¯æ²¡æœ‰æŠ¥å‘Šï¼Œä» messages ä¸­æå–
                    if not output:
                        for msg in result.get("messages", []):
                            if hasattr(msg, "content") and msg.content:
                                output = msg.content
                                break
        except Exception as e:
            print(f"âŒ åˆ†æ {ticker} å¤±è´¥: {e}")
            output = ""

        response_time_ms = int((time.time() - start_time) * 1000)
        
        # è¯„ä¼°å„é¡¹æŒ‡æ ‡
        completeness = self.check_completeness(output, mode)
        format_score = self.check_format_compliance(output, mode)
        accuracy = self.check_data_accuracy(output, ground_truth or {})
        recommendation, confidence = self.extract_recommendation(output)
        
        # ä¼°ç®— token æ•°ï¼ˆç®€åŒ–è®¡ç®—ï¼šä¸­æ–‡çº¦ 2 å­—ç¬¦/tokenï¼‰
        input_tokens = len(str(state)) // 2
        output_tokens = len(output) // 2
        
        # ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹ ID
        test_case_id = hashlib.md5(f"{ticker}_{date}_{prompt_version}".encode()).hexdigest()[:8]
        
        return EvaluationResult(
            test_case_id=test_case_id,
            prompt_version=prompt_version,
            ticker=ticker,
            date=date,
            completeness_score=completeness,
            format_compliance=format_score,
            data_accuracy=accuracy,
            input_tokens=input_tokens,
            output_tokens=output_tokens,
            response_time_ms=response_time_ms,
            recommendation=recommendation,
            confidence=confidence,
        )
    
    def run_ab_test(
        self,
        analyst_a,  # åŸç‰ˆåˆ†æå¸ˆ
        analyst_b,  # æ–°ç‰ˆåˆ†æå¸ˆ
        test_cases: List[Dict] = None,
        date: str = None,
        mode: str = "quick",
        toolkit = None
    ) -> pd.DataFrame:
        """
        è¿è¡Œ A/B æµ‹è¯•

        Args:
            analyst_a: åŸç‰ˆåˆ†æå¸ˆå‡½æ•°
            analyst_b: æ–°ç‰ˆåˆ†æå¸ˆå‡½æ•°
            test_cases: æµ‹è¯•ç”¨ä¾‹åˆ—è¡¨
            date: æµ‹è¯•æ—¥æœŸ
            mode: åˆ†ææ¨¡å¼
            toolkit: å·¥å…·é›†ï¼ˆç”¨äºæ‰§è¡Œå·¥å…·è°ƒç”¨ï¼‰

        Returns:
            å¯¹æ¯”ç»“æœ DataFrame
        """
        test_cases = test_cases or self.DEFAULT_TEST_CASES
        date = date or datetime.now().strftime("%Y-%m-%d")

        results_a = []
        results_b = []

        print(f"ğŸ”¬ å¼€å§‹ A/B æµ‹è¯•ï¼Œå…± {len(test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹")
        print("=" * 50)

        for i, case in enumerate(test_cases, 1):
            ticker = case["ticker"]
            name = case.get("name", ticker)

            print(f"\n[{i}/{len(test_cases)}] æµ‹è¯• {ticker} ({name})")

            # è¿è¡Œç‰ˆæœ¬ A
            print("  â”œâ”€ è¿è¡Œç‰ˆæœ¬ A (åŸç‰ˆ)...", end=" ")
            result_a = self.run_single_evaluation(
                analyst_a, ticker, date, "original", mode, toolkit=toolkit
            )
            results_a.append(result_a)
            print(f"å®Œæˆ (è€—æ—¶ {result_a.response_time_ms}ms)")

            # è¿è¡Œç‰ˆæœ¬ B
            print("  â””â”€ è¿è¡Œç‰ˆæœ¬ B (ä¼˜åŒ–ç‰ˆ)...", end=" ")
            result_b = self.run_single_evaluation(
                analyst_b, ticker, date, "optimized", mode, toolkit=toolkit
            )
            results_b.append(result_b)
            print(f"å®Œæˆ (è€—æ—¶ {result_b.response_time_ms}ms)")
        
        # æ±‡æ€»ç»“æœ
        df_a = pd.DataFrame([r.to_dict() for r in results_a])
        df_b = pd.DataFrame([r.to_dict() for r in results_b])
        
        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        summary = self._compute_summary(df_a, df_b)
        
        print("\n" + "=" * 50)
        print("ğŸ“Š A/B æµ‹è¯•ç»“æœæ±‡æ€»")
        print("=" * 50)
        print(summary.to_string())
        
        return summary
    
    def _compute_summary(self, df_a: pd.DataFrame, df_b: pd.DataFrame) -> pd.DataFrame:
        """è®¡ç®—æ±‡æ€»ç»Ÿè®¡"""
        metrics = [
            "completeness_score",
            "format_compliance", 
            "data_accuracy",
            "output_tokens",
            "response_time_ms",
        ]
        
        summary_data = []
        for metric in metrics:
            mean_a = df_a[metric].mean()
            mean_b = df_b[metric].mean()
            improvement = ((mean_b - mean_a) / mean_a * 100) if mean_a != 0 else 0
            
            summary_data.append({
                "æŒ‡æ ‡": metric,
                "åŸç‰ˆ (A)": f"{mean_a:.3f}",
                "ä¼˜åŒ–ç‰ˆ (B)": f"{mean_b:.3f}",
                "æå‡%": f"{improvement:+.1f}%",
            })
        
        return pd.DataFrame(summary_data)


class BacktestEvaluator:
    """
    å›æµ‹è¯„ä¼°å™¨
    ç”¨äºè¯„ä¼°åˆ†æå¸ˆå»ºè®®çš„å®é™…æŠ•èµ„æ•ˆæœ
    """
    
    def __init__(self, data_source=None):
        """
        Args:
            data_source: æ•°æ®æºï¼ˆå¦‚ akshare, tushareï¼‰
        """
        self.data_source = data_source
        
    def get_actual_return(self, ticker: str, date: str, hold_days: int) -> float:
        """
        è·å–å®é™…æ”¶ç›Šç‡
        
        Args:
            ticker: è‚¡ç¥¨ä»£ç 
            date: ä¹°å…¥æ—¥æœŸ
            hold_days: æŒæœ‰å¤©æ•°
            
        Returns:
            æ”¶ç›Šç‡ï¼ˆå¦‚ 0.05 è¡¨ç¤º 5%ï¼‰
        """
        # TODO: æ¥å…¥å®é™…æ•°æ®æº
        # ç¤ºä¾‹å®ç°
        try:
            import akshare as ak
            
            # è·å–è‚¡ç¥¨å†å²æ•°æ®
            df = ak.stock_zh_a_hist(
                symbol=ticker.split('.')[0],
                period="daily",
                start_date=date.replace('-', ''),
                adjust="qfq"
            )
            
            if len(df) < hold_days + 1:
                return 0.0
            
            buy_price = df.iloc[0]['æ”¶ç›˜']
            sell_price = df.iloc[hold_days]['æ”¶ç›˜']
            
            return (sell_price - buy_price) / buy_price
            
        except Exception as e:
            print(f"âš ï¸ è·å– {ticker} æ”¶ç›Šç‡å¤±è´¥: {e}")
            return 0.0
    
    def evaluate_recommendations(
        self, 
        evaluation_results: List[EvaluationResult],
        hold_days: int = 5
    ) -> pd.DataFrame:
        """
        è¯„ä¼°æŠ•èµ„å»ºè®®çš„å®é™…æ•ˆæœ
        
        Args:
            evaluation_results: è¯„ä¼°ç»“æœåˆ—è¡¨
            hold_days: æŒæœ‰å¤©æ•°
            
        Returns:
            åŒ…å«å®é™…æ”¶ç›Šçš„ DataFrame
        """
        results = []
        
        for eval_result in evaluation_results:
            actual_return = self.get_actual_return(
                eval_result.ticker,
                eval_result.date,
                hold_days
            )
            
            # è®¡ç®—ç­–ç•¥æ”¶ç›Š
            if eval_result.recommendation == "ä¹°å…¥":
                strategy_return = actual_return
            elif eval_result.recommendation == "å–å‡º":
                strategy_return = -actual_return  # åšç©ºæ”¶ç›Š
            else:
                strategy_return = 0  # æŒæœ‰ä¸æ“ä½œ
            
            results.append({
                "ticker": eval_result.ticker,
                "date": eval_result.date,
                "recommendation": eval_result.recommendation,
                "confidence": eval_result.confidence,
                "actual_return": actual_return,
                "strategy_return": strategy_return,
                "prompt_version": eval_result.prompt_version,
            })
        
        df = pd.DataFrame(results)
        
        # æŒ‰ç‰ˆæœ¬åˆ†ç»„ç»Ÿè®¡
        summary = df.groupby("prompt_version").agg({
            "strategy_return": ["mean", "std", "count"],
            "actual_return": "mean",
        }).round(4)
        
        print("\nğŸ“ˆ å›æµ‹ç»“æœæ±‡æ€»")
        print("=" * 50)
        print(summary)
        
        # è®¡ç®—èƒœç‡
        for version in df["prompt_version"].unique():
            version_df = df[df["prompt_version"] == version]
            win_rate = (version_df["strategy_return"] > 0).mean()
            print(f"\n{version} èƒœç‡: {win_rate:.1%}")
        
        return df


# ============================================================================
# ä½¿ç”¨ç¤ºä¾‹
# ============================================================================

def example_usage():
    """ä½¿ç”¨ç¤ºä¾‹"""
    
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘              Prompt æ•ˆæœè¯„ä¼°å·¥å…· - ä½¿ç”¨ç¤ºä¾‹                    â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    1. åŸºæœ¬ä½¿ç”¨æµç¨‹ï¼š
    
    ```python
    from prompt_evaluator import PromptEvaluator
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = PromptEvaluator()
    
    # å‡è®¾ä½ æœ‰ä¸¤ä¸ªåˆ†æå¸ˆå‡½æ•°
    from tradingagents.agents.analysts.china_market_analyst import create_china_market_analyst
    from china_market_analyst_optimized import create_quick_analyst
    
    # è¿è¡Œ A/B æµ‹è¯•
    results = evaluator.run_ab_test(
        analyst_a=original_analyst,
        analyst_b=optimized_analyst,
        mode="quick"
    )
    ```
    
    2. å›æµ‹è¯„ä¼°ï¼š
    
    ```python
    from prompt_evaluator import BacktestEvaluator
    
    backtest = BacktestEvaluator()
    backtest_results = backtest.evaluate_recommendations(
        evaluation_results,
        hold_days=5
    )
    ```
    
    3. è‡ªå®šä¹‰æµ‹è¯•ç”¨ä¾‹ï¼š
    
    ```python
    custom_cases = [
        {"ticker": "600519.SH", "name": "è´µå·èŒ…å°"},
        {"ticker": "000858.SZ", "name": "äº”ç²®æ¶²"},
    ]
    
    results = evaluator.run_ab_test(
        analyst_a, analyst_b,
        test_cases=custom_cases
    )
    ```
    """)


if __name__ == "__main__":
    example_usage()
